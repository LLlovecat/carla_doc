<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../../img/favicon.ico" />
    <title>Reinforcement Learning - 交通仿真文档</title>
    <link rel="stylesheet" href="../../../css/theme.css" />
    <link rel="stylesheet" href="../../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../../../extra.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Reinforcement Learning";
        var mkdocs_page_input_path = "algorithms\\DRL\\_index.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../../.." class="icon icon-home"> 交通仿真文档
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../..">主页</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../..">交通仿真文档</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../.." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Reinforcement Learning</li>
    <li class="wy-breadcrumbs-aside">
          <a href="https://github.com/OpenHUTB/carla_doc/edit/master/docs/algorithms/DRL/_index.md" class="icon icon-github"> Edit on GitHub</a>
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="deep-reinforcement-learning">Deep Reinforcement Learning</h1>
<p>Deep Reinforcement Learning (DRL) is an area of machine learning that focuses on training agents to make sequences of decisions by interacting with an environment. The agent seeks to learn the best strategy, or policy, to achieve the maximum cumulative reward over time. DRL has shown impressive results in various domains, from game playing to robotic control.</p>
<p><img alt="RL_pipeline" src="../../assets/RL_pipeline.png" />{: .center-image width="70%"}</p>
<p><em>Reinforcement Learning Pipeline</em>
{: .text-center}</p>
<h2 id="our-plan">Our Plan</h2>
<h3 id="1-problem-formulation">1. Problem Formulation</h3>
<p>Define the specific navigation tasks we want the robot to accomplish such as waypoint following, obstacle avoidance, etc. We will then set up the reward structure to encourage these desired behaviors.</p>
<h3 id="2-simulator-setup">2. Simulator Setup</h3>
<p>We will be using an appropriate simulator for Ackermann robots, possibly Gazebo, Webots, or even a custom simulator. The goal is to model the robot's dynamics and sensory inputs as accurately as possible.</p>
<h3 id="3-baseline-drl-algorithm-selection">3. Baseline DRL Algorithm Selection</h3>
<p>Our starting point will be popular reinforcement learning algorithms like Proximal Policy Optimization (PPO), Deep Q-Network (DQN), or Twin Delayed Deep Deterministic Policy Gradient (TD3).</p>
<p>We are going to use <a href="https://github.com/DLR-RM/stable-baselines3">Stable Baselines3</a> for our DRL algorithms. Stable Baselines3 is a set of improved implementations of reinforcement learning algorithms in PyTorch.</p>
<h3 id="4-training-in-simulation">4. Training in Simulation</h3>
<p>Starting with a basic environment (few obstacles, clear paths), we will progress to more intricate scenarios. We will monitor the agent's training progress and tune hyperparameters for optimal performance. A few simulators we are considering are listed below:</p>
<table>
<thead>
<tr>
<th><strong>Simulator</strong></th>
<th><strong>Strengths</strong></th>
<th><strong>Weaknesses</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Gazebo (ROS)</strong></td>
<td>- Easy sim-to-real experiments. <br> - Integrates well with the ROS ecosystem.</td>
<td>- Slow simulation speeds. <br> - Requires proficiency in ROS.</td>
</tr>
<tr>
<td><strong>Isaac Sim (NVIDIA)</strong></td>
<td>- GPU acceleration for fast training. <br> - High-quality graphics.</td>
<td>- Steeper learning curve. <br> - May require advanced hardware.</td>
</tr>
<tr>
<td><strong>Carla</strong></td>
<td>- Complex sensor data. <br> - Real vehicle physics.</td>
<td>- Slow simulations. <br> - High computational resource needs.</td>
</tr>
<tr>
<td><strong>SMARTS (Huawei)</strong></td>
<td>- Great visualization. <br> - Broad action space for RL in driving.</td>
<td>- Might be challenging for newcomers. <br> - Specific environment understanding required.</td>
</tr>
</tbody>
</table>
<h3 id="5-transfer-learning-and-domain-adaptation">5. Transfer Learning and Domain Adaptation</h3>
<p>Given the known challenges of the sim-to-real gap, we will implement techniques like domain randomization in the simulator and consider fine-tuning on the real robot.</p>
<h3 id="6-safety-precautions">6. Safety Precautions</h3>
<p>Safety is paramount. When transitioning to a real robot, we'll ensure protocols are in place to protect both the robot and its surroundings.</p>
<h3 id="7-evaluation">7. Evaluation</h3>
<p>We will set up evaluation metrics to gauge the performance of our agent in real-world settings, comparing different algorithms to identify the most suitable one.</p>
<h3 id="8-iterate-and-improve">8. Iterate and Improve</h3>
<p>Using feedback from real-world testing, we'll refine our simulation, reward designs, and DRL algorithms to continually enhance performance.</p>
              
            </div>
          </div><footer>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
        <span>
          <a href="https://github.com/OpenHUTB/carla_doc" class="fa fa-github" style="color: #fcfcfc"> GitHub</a>
        </span>
    
    
    
  </span>
</div>
    <script src="../../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../../..";</script>
    <script src="../../../js/theme_extra.js"></script>
    <script src="../../../js/theme.js"></script>
      <script src="../../../extra.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      <script src="../../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
